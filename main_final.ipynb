{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97123060-78a6-4369-a1fa-e2f1157e0a72",
   "metadata": {},
   "source": [
    "# Create a graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6298ee85-d8b8-4137-bf2b-a937e68dbcfe",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da6908e-3b00-4746-8033-79ca871391fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import powerlaw\n",
    "import os\n",
    "import scipy.sparse\n",
    "from scipy.sparse.linalg import eigs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib.image import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9931b63-3b3d-4de6-b63a-429bab3f6622",
   "metadata": {},
   "source": [
    "Parameters of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dca4a8bb-f2af-4d67-b3d4-d60b31ba1a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_nodes = 1000\n",
    "probability = 0.01  # Probability of edge creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c3de69-cffc-4662-8abc-7d8b3362fb63",
   "metadata": {},
   "source": [
    "Create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "decdd09e-5fbb-4f5e-981b-094458ee70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_name = \"ER_graph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d5c5cacc-0457-49bf-8159-07b48fdc8f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an Erdős–Rényi graph\n",
    "G = nx.erdos_renyi_graph(n=num_nodes, p=probability)\n",
    "#G = nx.powerlaw_cluster_graph(n=num_nodes, m=5, p=0.3)\n",
    "# Assign random weights to the edges, rounded to 3 decimals\n",
    "for u, v in G.edges():\n",
    "    G[u][v]['weight'] = round(random.uniform(0.1, 10.0), 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5fc811",
   "metadata": {},
   "outputs": [],
   "source": [
    "bet = nx.edge_betweenness_centrality(G,weight=\"weight\", backend=\"parallel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8addf910-c9ea-487d-a96d-2c342984e52e",
   "metadata": {},
   "source": [
    "Save the network as edgelist for reproducibility of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e6923513-c5e4-4d2e-96a3-f4fd3195b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./networks/\"\n",
    "output_file = os.path.join(output_dir, f\"{graph_name}.edgelist\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "nx.write_weighted_edgelist(G, f\"./networks/{graph_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e920d4a3-e14c-4bc6-beee-d038097d5282",
   "metadata": {},
   "source": [
    "# Read the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e7ae06-f86f-431b-ba31-12e842380bf8",
   "metadata": {},
   "source": [
    "Read the graph from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4846d2ae-70c2-419a-bc56-804d2eadb3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_name = \"ER_graph\"\n",
    "input_file = f\"./networks/{syn_name}\"\n",
    "\n",
    "# Read the weighted graph\n",
    "syn = nx.read_weighted_edgelist(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba870e7d",
   "metadata": {},
   "source": [
    "Function to read txt graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eef2e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weighted_graph(file_path):\n",
    "    G = nx.Graph()  \n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:  \n",
    "            parts = line.split()\n",
    "            if len(parts) == 3:\n",
    "                node1 = int(parts[0])\n",
    "                node2 = int(parts[1])\n",
    "                weight = float(parts[2])\n",
    "                G.add_edge(node1, node2, weight=weight)\n",
    "    return G\n",
    "\n",
    "# file paths\n",
    "file_path_karate = 'networks/karate.txt'\n",
    "file_path_kangaroos = 'networks/Kangaroos.txt'\n",
    "\n",
    "karate = load_weighted_graph(file_path_karate)\n",
    "kangaroos = load_weighted_graph(file_path_kangaroos)\n",
    "# names \n",
    "karate_name = \"karate_graph\"\n",
    "kangaroos_name = \"kangaroos_graph\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af56850f-3881-4cf2-9e5b-3e4122e7c8ab",
   "metadata": {},
   "source": [
    "Check they are properly read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fc1f8c-64ca-44c5-93b9-886c09f5b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some edges with weights to verify\n",
    "print(\"Sample edges from the synthetic graph:\")\n",
    "for u, v, data in list(syn.edges(data=True))[:10]:\n",
    "    print(f\"({u}, {v}) - weight: {data['weight']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some edges with weights to verify\n",
    "print(\"Sample edges from the karate graph:\")\n",
    "for u, v, data in list(karate.edges(data=True))[:10]:\n",
    "    print(f\"({u}, {v}) - weight: {data['weight']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some edges with weights to verify\n",
    "print(\"Sample edges from the kangaroos graph:\")\n",
    "for u, v, data in list(kangaroos.edges(data=True))[:10]:\n",
    "    print(f\"({u}, {v}) - weight: {data['weight']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae998c1-222d-4de2-a3ed-0aabd24d93db",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59380488-063f-41b9-a6bd-481c77464de8",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52efa78a-8952-4021-b896-308af303e9a5",
   "metadata": {},
   "source": [
    "### Explore the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e5dbf994-550e-4f5d-ac2b-0db6f7a496f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_graph_properties(graph, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Basic properties\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    num_edges = graph.number_of_edges()\n",
    "    edge_weights = [d['weight'] for _, _, d in graph.edges(data=True)]\n",
    "    min_weight = min(edge_weights)\n",
    "    max_weight = max(edge_weights)\n",
    "    avg_weight = sum(edge_weights) / len(edge_weights)\n",
    "    \n",
    "    # Graph density\n",
    "    density = nx.density(graph)\n",
    "    \n",
    "    # Degree distribution\n",
    "    degrees = [d for _, d in graph.degree()]\n",
    "    weighted_degrees = [sum(graph[u][v]['weight'] for v in graph.neighbors(u)) for u in graph.nodes()]\n",
    "    \n",
    "    # Connected components\n",
    "    connected_components = list(nx.connected_components(graph))\n",
    "    num_components = len(connected_components)\n",
    "    component_sizes = [len(c) for c in connected_components]\n",
    "    \n",
    "    # Clustering coefficient\n",
    "    clustering_coeff = nx.average_clustering(graph, weight='weight')\n",
    "    \n",
    "    # Additional properties\n",
    "    # Average path length (weighted)\n",
    "    avg_path_length = nx.average_shortest_path_length(graph, weight='weight') if nx.is_connected(graph) else None\n",
    "    \n",
    "    # Diameter (weighted)\n",
    "    diameter = nx.diameter(graph, weight='weight') if nx.is_connected(graph) else None\n",
    "    \n",
    "    # Radius (weighted)\n",
    "    radius = nx.radius(graph, weight='weight') if nx.is_connected(graph) else None\n",
    "    \n",
    "    # Degree assortativity\n",
    "    degree_assortativity = nx.degree_assortativity_coefficient(graph)\n",
    "    \n",
    "    # Edge connectivity\n",
    "    edge_connectivity = nx.edge_connectivity(graph)\n",
    "    \n",
    "    # Compute the largest eigenvalue of the adjacency matrix\n",
    "    adj_matrix = nx.to_numpy_array(graph, weight='weight')\n",
    "    eigenvalues = np.linalg.eigvals(adj_matrix)\n",
    "    first_eigenvalue = max(eigenvalues)  # Get the largest eigenvalue\n",
    "    \n",
    "    # Prepare the results for saving to file\n",
    "    results = [\n",
    "        f\"Number of nodes: {num_nodes}\",\n",
    "        f\"Number of edges: {num_edges}\",\n",
    "        f\"Minimum edge weight: {min_weight}\",\n",
    "        f\"Maximum edge weight: {max_weight}\",\n",
    "        f\"Average edge weight: {avg_weight:.2f}\",\n",
    "        f\"Graph density: {density:.4f}\",\n",
    "        f\"Number of connected components: {num_components}\",\n",
    "        f\"Sizes of connected components: {component_sizes}\",\n",
    "        f\"Global clustering coefficient: {clustering_coeff:.4f}\",\n",
    "        f\"Average path length: {avg_path_length}\",\n",
    "        f\"Diameter: {diameter}\",\n",
    "        f\"Radius: {radius}\",\n",
    "        f\"Degree assortativity: {degree_assortativity:.4f}\",\n",
    "        f\"Edge connectivity: {edge_connectivity}\",\n",
    "        f\"First eigenvalue: {first_eigenvalue:.6f}\"  # Add the first eigenvalue\n",
    "    ]\n",
    "    \n",
    "    # Save the results to a text file\n",
    "    results_file = os.path.join(output_dir, \"graph_properties.txt\")\n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(\"\\n\".join(results))\n",
    "    \n",
    "    # Display the contents of the text file\n",
    "    with open(results_file, 'r') as f:\n",
    "        print(f.read())\n",
    "        \n",
    "    # Plot edge weight distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(edge_weights, bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.title('Edge Weight Distribution', fontsize=14)\n",
    "    plt.xlabel('Weight', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"edge_weight_distribution.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot degree distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(degrees, bins=20, color='orange', edgecolor='black')\n",
    "    plt.title('Degree Distribution', fontsize=14)\n",
    "    plt.xlabel('Degree', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"degree_distribution.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Plot weighted degree distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(weighted_degrees, bins=20, color='green', edgecolor='black')\n",
    "    plt.title('Weighted Degree Distribution', fontsize=14)\n",
    "    plt.xlabel('Weighted Degree', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"weighted_degree_distribution.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a3d5f4-75b4-4b57-b0c2-c3dd2b8dafc6",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9e69a-3390-4cac-b293-fe69ce6de500",
   "metadata": {},
   "source": [
    "Get the largest eigenvalue from a given adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d93901b-cf30-4f10-b7ce-be41231dbe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_eigenval(\n",
    "    adj_matrix: scipy.sparse._csr.csr_array\n",
    ") -> np.float64:\n",
    "    eigenvalues, _ = eigs(adj_matrix, k=1, which='LM')  # 'LM' for largest magnitude eigenvalue\n",
    "    # The largest eigenvalue is the real part of the first eigenvalue\n",
    "    largest_eigenvalue = np.real(eigenvalues[0])\n",
    "    return largest_eigenvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7f0bde3-8fc2-45f7-952b-97a1c50d8819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqrt_trace_of_adj_squared(\n",
    "    adj_matrix: scipy.sparse._csr.csr_array\n",
    ") -> np.float64:\n",
    "    # Ensure the matrix is square\n",
    "    if adj_matrix.shape[0] != adj_matrix.shape[1]:\n",
    "        raise ValueError(\"The adjacency matrix must be square.\")\n",
    "    \n",
    "    # Compute adj_matrix^2\n",
    "    adj_squared = adj_matrix @ adj_matrix  # Efficient sparse matrix multiplication\n",
    "    \n",
    "    # Compute the trace of adj_matrix^2\n",
    "    trace = adj_squared.diagonal().sum()  # Sum of diagonal elements\n",
    "    \n",
    "    # Compute the square root of the trace\n",
    "    result = np.sqrt(trace)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b707c58f-0b34-4e8c-aea9-381c98d89318",
   "metadata": {},
   "source": [
    "Get the amount of change (psi) by using spectral norm ||A||_2 = λ_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8e7bbcb-3179-4974-bb4f-4a8d29a9dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_amount_of_change(\n",
    "    perturbed_adj_matrix: scipy.sparse._csr.csr_array, \n",
    "    adj_matrix: scipy.sparse._csr.csr_array\n",
    ") -> np.float64:\n",
    "    perturbation_matrix = perturbed_adj_matrix - adj_matrix \n",
    "    return sqrt_trace_of_adj_squared(perturbation_matrix)/sqrt_trace_of_adj_squared(adj_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b0647c-11a7-4e02-b503-85cd7e73560e",
   "metadata": {},
   "source": [
    "#### Define centrality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e9be866-6d70-4feb-9fff-80c1688efb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_centrality(\n",
    "    adj_matrix: scipy.sparse._csr.csr_array\n",
    ") -> dict:\n",
    "    adj_matrix_dense = adj_matrix.todense()\n",
    "    # Convert the dense adjacency matrix to a NetworkX graph\n",
    "    G_from_adj = nx.from_numpy_array(np.array(adj_matrix_dense))\n",
    "    # Compute degree centrality using networkx\n",
    "    degree_centrality = {node: degree for node, degree in G_from_adj.degree(weight='weight')}\n",
    "    return degree_centrality\n",
    "\n",
    "def weighted_katz_centrality(\n",
    "    adj_matrix: scipy.sparse._csr.csr_array\n",
    ") -> dict:\n",
    "    adj_matrix_dense = adj_matrix.todense()\n",
    "    # Convert the dense adjacency matrix to a NetworkX graph\n",
    "    G_from_adj = nx.from_numpy_array(np.array(adj_matrix_dense))\n",
    "    # get the alpha value \n",
    "    max_alpha = 1/get_largest_eigenval(adj_matrix)\n",
    "    alpha = max_alpha * 0.9 #we reduce max alpha by 10%\n",
    "    # Compute degree centrality using networkx\n",
    "    katz_centrality = nx.katz_centrality(G_from_adj, alpha=alpha, weight = \"weight\")\n",
    "    return katz_centrality\n",
    "\n",
    "def weighted_eigenvector_centrality(\n",
    "    adj_matrix: scipy.sparse._csr.csr_array\n",
    ") -> dict:\n",
    "    adj_matrix_dense = adj_matrix.todense()\n",
    "    # Convert the dense adjacency matrix to a NetworkX graph\n",
    "    G_from_adj = nx.from_numpy_array(np.array(adj_matrix_dense))\n",
    "    # Compute degree centrality using networkx\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(G_from_adj, weight = \"weight\")\n",
    "    return eigenvector_centrality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503650a-8eac-4c38-9f5e-0d802ae9646e",
   "metadata": {},
   "source": [
    "Wrap the computation of different centrality metrics in a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "050b6707-2d9a-4e32-b77f-94601adadfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centrality_scores(\n",
    "    adj_matrix: scipy.sparse._csr.csr_array, \n",
    "    func\n",
    ") -> dict:\n",
    "    return func(adj_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef84f1d-1e20-4d35-a110-09a92b3288c0",
   "metadata": {},
   "source": [
    "Get the deformation of centrality metrics (ζ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "266cae4b-18ee-4255-a624-dac24b160ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deformation_centrality_metric(\n",
    "    perturbed_adj_matrix: scipy.sparse._csr.csr_array, \n",
    "    adj_matrix: scipy.sparse._csr.csr_array, \n",
    "    func\n",
    ") -> np.float64:\n",
    "    centrality_metric_adj_matrix = get_centrality_scores(adj_matrix, func)\n",
    "    centrality_metric_perturbed_adj_matrix  = get_centrality_scores(perturbed_adj_matrix, func)\n",
    "    diff_centrality_metric_dict = {\n",
    "        key: centrality_metric_perturbed_adj_matrix[key] - centrality_metric_adj_matrix[key] \n",
    "        for key in centrality_metric_perturbed_adj_matrix.keys()\n",
    "    }\n",
    "    euclidean_norm_diff = np.sqrt(sum(value**2 for value in diff_centrality_metric_dict.values()))\n",
    "    euclidean_norm = np.sqrt(sum(value**2 for value in centrality_metric_adj_matrix.values()))\n",
    "    return euclidean_norm_diff/euclidean_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197addbe-0c47-43a4-aa3d-0b35a6f1001c",
   "metadata": {},
   "source": [
    "#### Define perturbation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdfda0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_based_on_edge_betweenness(\n",
    "    G_sparse: nx.classes.graph.Graph,\n",
    "    E_prime: dict,\n",
    "    edge_betweenness: dict,\n",
    "    consider_only_E_prime: bool = False,\n",
    "    scaling: str='Min-Max'\n",
    ") -> scipy.sparse._csr.csr_array:\n",
    "    G_new = G_sparse.copy()\n",
    "    E_prime_edges = list(E_prime.keys())\n",
    "    filtered_edges = {edge: edge_betweenness.get(edge, 0) for edge in E_prime_edges} if consider_only_E_prime else edge_betweenness\n",
    "    if scaling == 'Min-Max':\n",
    "        min_edge_betweenness = min(filtered_edges.values())\n",
    "        max_edge_betweenness = max(filtered_edges.values())\n",
    "        if max_edge_betweenness > min_edge_betweenness:\n",
    "            normalized_centrality = np.array([\n",
    "                (filtered_edges[edge] - min_edge_betweenness) / (max_edge_betweenness - min_edge_betweenness)\n",
    "                for edge in E_prime_edges\n",
    "            ]) \n",
    "    else:\n",
    "        total_betweenness = sum(filtered_edges.values())\n",
    "        normalized_centrality = np.array([filtered_edges[edge] / total_betweenness for edge in E_prime_edges])\n",
    "    random_values = np.random.rand(len(E_prime_edges))\n",
    "    mask = random_values < normalized_centrality\n",
    "    edges_to_remove = np.array(E_prime_edges)[mask]\n",
    "    G_new.remove_edges_from(edges_to_remove)\n",
    "\n",
    "    return nx.adjacency_matrix(G_new)\n",
    "\n",
    "\n",
    "def perturb_based_on_weight(\n",
    "    G_sparse: nx.classes.graph.Graph,\n",
    "    E_prime: dict,\n",
    "    edge_weights = dict,\n",
    "    consider_only_E_prime: bool = False,\n",
    "    scaling: str = 'Min-Max'\n",
    ") -> scipy.sparse._csr.csr_array:\n",
    "    G_new = G_sparse.copy()\n",
    "    E_prime_edges = list(E_prime.keys())\n",
    "    filtered_edges = {edge: E_prime[edge] for edge in E_prime_edges} if consider_only_E_prime else edge_weights\n",
    "    if scaling == 'Min-Max':\n",
    "        min_weight = min(filtered_edges.values())\n",
    "        max_weight = max(filtered_edges.values())\n",
    "        if max_weight > min_weight:\n",
    "            normalized_weight = np.array([\n",
    "                (filtered_edges[edge] - min_weight) / (max_weight - min_weight)\n",
    "                for edge in E_prime_edges\n",
    "            ])\n",
    "    else:\n",
    "        total_weight = sum(filtered_edges.values())\n",
    "        normalized_weight = np.array([filtered_edges[edge] / total_weight for edge in E_prime_edges])\n",
    "    random_values = np.random.rand(len(E_prime_edges))\n",
    "    mask = random_values < normalized_weight\n",
    "    edges_to_remove = np.array(E_prime_edges)[mask]\n",
    "    G_new.remove_edges_from(edges_to_remove)\n",
    "\n",
    "    return nx.adjacency_matrix(G_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746ffcfa-f963-4539-804b-3d6979eedf1d",
   "metadata": {},
   "source": [
    "Wrap the methods to perturb a matrix in a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b7406ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_adj_matrix(\n",
    "    G_sparse: nx.classes.graph.Graph,\n",
    "    E_prime: dict,\n",
    "    edge_betweenness: dict = None,\n",
    "    edge_weights: dict = None,\n",
    "    func: str = 'betweenness',\n",
    "    scaling: str = 'Min-Max',\n",
    "    consider_only_E_prime: bool = False\n",
    "):\n",
    "    if func == 'betweenness':\n",
    "        if edge_betweenness is None:\n",
    "            raise ValueError(\"edge_betweenness must be provided when func is 'betweenness'\")\n",
    "        return perturb_based_on_edge_betweenness(\n",
    "            G_sparse, E_prime, edge_betweenness, consider_only_E_prime, scaling\n",
    "        )\n",
    "    elif func == 'weight':\n",
    "        if edge_weights is None:\n",
    "            raise ValueError(\"edge_weights must be provided when func is 'weight'\")\n",
    "        return perturb_based_on_weight(\n",
    "            G_sparse, E_prime, edge_weights, consider_only_E_prime, scaling\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3131811d-4872-4484-aed2-9e3d9f9d68d3",
   "metadata": {},
   "source": [
    "#### Subset E'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bee5509-2a11-4aaf-9efe-e05bf1d01c73",
   "metadata": {},
   "source": [
    "Get the subset of edges that can fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "531d4bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges_subset(G_loaded, fraction: float) -> dict:\n",
    "    # Get all edges with weights from the graph and store in a dictionary\n",
    "    edges_with_weights = {(u, v): data['weight'] for u, v, data in G_loaded.edges(data=True)}\n",
    "    # Calculate the number of edges to select based on the fraction\n",
    "    num_edges_to_select = int(len(edges_with_weights) * fraction)\n",
    "    # Randomly sample\n",
    "    random_edges = random.sample(list(edges_with_weights.items()), num_edges_to_select)\n",
    "    random_edges_dict = dict(random_edges)\n",
    "    \n",
    "    return random_edges_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf13920-16a9-494b-971d-26b81ccb45c8",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba654d57-0f66-4700-aeab-f3cfe01c4935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(\n",
    "    x_list: list, \n",
    "    y_list: list, \n",
    "    x_label: str, \n",
    "    y_label: str, \n",
    "    title: str,\n",
    "    plot_linear_regression: bool = False, \n",
    "    save_path: str = None\n",
    "):\n",
    "    x_array = np.array(x_list)\n",
    "    y_array = np.array(y_list)\n",
    "    X = x_array.reshape(-1, 1)\n",
    "    y = y_array\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    slope = model.coef_[0]\n",
    "    intercept = model.intercept_\n",
    "    x_range = np.linspace(min(x_array), max(x_array), 500).reshape(-1, 1)\n",
    "    y_pred = model.predict(x_range)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(x_array, y_array, color='blue', alpha=0.7, label='Data Points')\n",
    "    if plot_linear_regression:\n",
    "        plt.plot(x_range, y_pred, color='red', label='Regression Line', linewidth=2)\n",
    "        plt.text(\n",
    "            0.05, 0.95,\n",
    "            fr'$y = {slope:.3e}x + {intercept:.3e}$',\n",
    "            fontsize=12,\n",
    "            transform=plt.gca().transAxes,\n",
    "            verticalalignment='top'\n",
    "        )\n",
    "    plt.title(fr'{title}', fontsize=16)\n",
    "    plt.xlabel(fr'{x_label}', fontsize=14)\n",
    "    plt.ylabel(fr'{y_label}', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def display_proportional_grid(directory: str):\n",
    "    plot_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    if not plot_files:\n",
    "        print(f\"No plots found in the directory: {directory}\")\n",
    "        return\n",
    "    \n",
    "    # Determine grid dimensions\n",
    "    num_plots = len(plot_files)\n",
    "    rows = 1 if num_plots <= 3 else int(num_plots**0.5)  # Ensure at least one row\n",
    "    cols = (num_plots + rows - 1) // rows  # Ensure all plots fit in the grid\n",
    "\n",
    "    # Create a grid of plots\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, plot_file in zip(axes, plot_files):\n",
    "        img = imread(plot_file)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')  # Remove axis\n",
    "\n",
    "    # Turn off any unused axes\n",
    "    for ax in axes[len(plot_files):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca9e4c0",
   "metadata": {},
   "source": [
    "### all plots toghter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5b8576f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_results(\n",
    "    x_list: list,  # x is the same for all datasets\n",
    "    y_lists: list,  # List of y data lists\n",
    "    labels: list,\n",
    "    x_label: str,\n",
    "    y_label: str,\n",
    "    title: str,\n",
    "    plot_linear_regression: bool = False,\n",
    "    save_path: str = None\n",
    "):\n",
    "    x_array = np.array(x_list)\n",
    "    X = x_array.reshape(-1, 1)\n",
    "    \n",
    "    colors = ['blue', 'green', 'orange']  # Define a set of colors\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    for i, (y_list, label) in enumerate(zip(y_lists, labels)):\n",
    "        y_array = np.array(y_list)\n",
    "        \n",
    "        plt.plot(\n",
    "            x_array, y_array, alpha=0.7, label=label, color=colors[i % len(colors)],\n",
    "            marker='o', linestyle='-', linewidth=1.5  # Marker for points and line style\n",
    "        )\n",
    "        \n",
    "        if plot_linear_regression:\n",
    "            # Linear regression\n",
    "            model = LinearRegression()\n",
    "            model.fit(X, y_array)\n",
    "            slope = model.coef_[0]\n",
    "            intercept = model.intercept_\n",
    "            x_range = np.linspace(min(x_array), max(x_array), 500).reshape(-1, 1)\n",
    "            y_pred = model.predict(x_range)\n",
    "            plt.plot(\n",
    "                x_range, y_pred,\n",
    "                label=f'Regression {i+1}: y = {slope:.3e}x + {intercept:.3e}',\n",
    "                color=colors[i % len(colors)],\n",
    "                linewidth=2,\n",
    "                linestyle='--'\n",
    "            )\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel(x_label, fontsize=14)\n",
    "    plt.ylabel(y_label, fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def display_proportional_grid(directory: str):\n",
    "    # Collect all image files in the directory\n",
    "    plot_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    if not plot_files:\n",
    "        print(f\"No plots found in the directory: {directory}\")\n",
    "        return\n",
    "    \n",
    "    # Determine grid dimensions\n",
    "    num_plots = len(plot_files)\n",
    "    rows = int(num_plots**0.5)  # Number of rows (square root for proportional grid)\n",
    "    cols = (num_plots + rows - 1) // rows  # Adjust number of columns to fit all images\n",
    "\n",
    "    # Create a grid of plots\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, plot_file in zip(axes, plot_files):\n",
    "        img = imread(plot_file)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')  # Remove axis for clean display\n",
    "\n",
    "    # Turn off any unused axes\n",
    "    for ax in axes[len(plot_files):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c39159d-a8db-432c-874d-649965c3a4c2",
   "metadata": {},
   "source": [
    "# Set configuration"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f5c751b-166b-429e-bd22-19c0beae4d6f",
   "metadata": {},
   "source": [
    "# enabling networkx's config for nx-parallel\n",
    "nx.config.backends.parallel.active = True\n",
    "nx.config.warnings_to_ignore.add(\"cache\") #nx.config.warnings_to_ignore.remove(\"cache\")\n",
    "\n",
    "# Setting global configs\n",
    "nxp_config = nx.config.backends.parallel\n",
    "nxp_config.n_jobs = 3\n",
    "nxp_config.verbose = 50\n",
    "print(nx.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd52419-f1cc-4bdc-ac00-4752614ed5aa",
   "metadata": {},
   "source": [
    "# Explore the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e92513",
   "metadata": {},
   "source": [
    "synthetic graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7511b5df-6e3c-401f-83cb-d57fedf40a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_syn = f\"properties/{syn_name}\"\n",
    "save_graph_properties(syn, output_dir_syn)\n",
    "display_proportional_grid(output_dir_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad30500",
   "metadata": {},
   "source": [
    "karate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed1365",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_karate = f\"properties/{karate_name}\"\n",
    "save_graph_properties(karate, output_dir_karate)\n",
    "display_proportional_grid(output_dir_karate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f27c5",
   "metadata": {},
   "source": [
    "kangaroos graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir_kangaroos = f\"properties/{kangaroos_name}\"\n",
    "save_graph_properties(kangaroos, output_dir_kangaroos)\n",
    "display_proportional_grid(output_dir_kangaroos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12500e52-cb2b-4418-944f-70b2efc07195",
   "metadata": {},
   "source": [
    "# Init experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28caa1f9",
   "metadata": {},
   "source": [
    "### Define adj matrix, edge_betweenness, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f935dcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#synthetic\n",
    "adj_matrix_syn = nx.adjacency_matrix(syn)\n",
    "edge_betweenness_syn = nx.edge_betweenness_centrality(syn, weight=\"weight\")#, backend = \"parallel\")\n",
    "edge_weights_syn = {(u, v): data['weight'] for u, v, data in syn.edges(data=True)}\n",
    "#enron\n",
    "adj_matrix_karate = nx.adjacency_matrix(karate)\n",
    "edge_betweenness_karate = nx.edge_betweenness_centrality(karate, weight=\"weight\")#, backend = \"parallel\")\n",
    "edge_weights_karate = {(u, v): data['weight'] for u, v, data in karate.edges(data=True)}\n",
    "#kangaroos\n",
    "adj_matrix_kangaroos = nx.adjacency_matrix(kangaroos)\n",
    "edge_betweenness_kangaroos = nx.edge_betweenness_centrality(kangaroos, weight=\"weight\")#, backend = \"parallel\")\n",
    "edge_weights_kangaroos = {(u, v): data['weight'] for u, v, data in kangaroos.edges(data=True)}\n",
    "# tau list for all\n",
    "tau_list = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# number of iteration for all\n",
    "n_iter = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf7ee65-96ab-4313-9d4c-4e0351853c5b",
   "metadata": {},
   "source": [
    "Run the experiment for different taus and graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61e90c",
   "metadata": {},
   "source": [
    "### Synthetic graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c95bdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = \"betweenness\"\n",
    "scaling = \"Min-Max\"\n",
    "\n",
    "if func == \"betweenness\":\n",
    "    edge_betweenness_dict = edge_betweenness_syn\n",
    "    edge_weights_dict = None\n",
    "elif func == \"weight\":\n",
    "    edge_betweenness_dict = None\n",
    "    edge_weights_dict = edge_weights_syn\n",
    "    \n",
    "\n",
    "psi_list_syn = []\n",
    "zeta_degree_centrality_list_syn = []\n",
    "zeta_katz_centrality_list_syn = []\n",
    "zeta_eigenvector_centrality_list_syn = []\n",
    "\n",
    "for tau in tau_list:\n",
    "    print(f\"Processing tau: {tau}\")\n",
    "    psi_cumm = 0\n",
    "    zeta_degree_centrality_cumm = 0\n",
    "    zeta_closeness_centrality_cumm = 0\n",
    "    zeta_katz_centrality_cumm = 0\n",
    "    zeta_eigenvector_centrality_cumm = 0\n",
    "    for _ in range(n_iter):\n",
    "        E_prime = get_edges_subset(syn, tau)\n",
    "\n",
    "        '''Type of perturbation: func = 'betweenness' or func= 'weight'\n",
    "           Object to be pass w.r.t type of perturbation: edge_weights=edge_weights else edge_betweenness = edge_betweenness (just one the other set to None as default)\n",
    "           Type of Normalization: scaling = 'Min-Max' or scaling = 'Sum'\n",
    "           condider only the subset for removal: consider_only_E_prime = True or false\n",
    "        '''\n",
    "        perturbed_adj_matrix = perturb_adj_matrix(\n",
    "            syn, \n",
    "            E_prime, \n",
    "            edge_weights = edge_weights_dict,\n",
    "            edge_betweenness = edge_betweenness_dict, \n",
    "            func = func, \n",
    "            scaling = scaling, \n",
    "            consider_only_E_prime = False\n",
    "        ) \n",
    "        psi_cumm += get_amount_of_change(perturbed_adj_matrix, adj_matrix_syn)\n",
    "        zeta_degree_centrality_cumm += get_deformation_centrality_metric(perturbed_adj_matrix, adj_matrix_syn, degree_centrality) \n",
    "        zeta_katz_centrality_cumm += get_deformation_centrality_metric(perturbed_adj_matrix, adj_matrix_syn, weighted_katz_centrality) \n",
    "        zeta_eigenvector_centrality_cumm += get_deformation_centrality_metric(perturbed_adj_matrix, adj_matrix_syn, weighted_eigenvector_centrality) \n",
    "        \n",
    "    \n",
    "    mean_psi = psi_cumm/n_iter\n",
    "    mean_zeta_degree_centrality = zeta_degree_centrality_cumm/n_iter\n",
    "    mean_zeta_katz_centrality = zeta_katz_centrality_cumm/n_iter\n",
    "    mean_zeta_eigenvector_centrality = zeta_eigenvector_centrality_cumm/n_iter\n",
    "\n",
    "    psi_list_syn.append(mean_psi)\n",
    "    zeta_degree_centrality_list_syn.append(mean_zeta_degree_centrality)\n",
    "    zeta_katz_centrality_list_syn.append(mean_zeta_katz_centrality)\n",
    "    zeta_eigenvector_centrality_list_syn.append(mean_zeta_eigenvector_centrality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2403a2",
   "metadata": {},
   "source": [
    "### Karate graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbce0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = \"betweenness\"\n",
    "scaling = \"Min-Max\"\n",
    "\n",
    "if func == \"betweenness\":\n",
    "    edge_betweenness_dict = edge_betweenness_karate\n",
    "    edge_weights_dict = None\n",
    "elif func == \"weight\":\n",
    "    edge_betweenness_dict = None\n",
    "    edge_weights_dict = edge_weights_karate\n",
    "    \n",
    "\n",
    "psi_list_karate = []\n",
    "zeta_degree_centrality_list_karate = []\n",
    "zeta_katz_centrality_list_karate = []\n",
    "zeta_eigenvector_centrality_list_karate = []\n",
    "\n",
    "for tau in tau_list:\n",
    "    print(f\"Processing tau: {tau}\")\n",
    "    psi_cumm = 0\n",
    "    zeta_degree_centrality_cumm = 0\n",
    "    zeta_closeness_centrality_cumm = 0\n",
    "    zeta_katz_centrality_cumm = 0\n",
    "    zeta_eigenvector_centrality_cumm = 0\n",
    "    for _ in range(n_iter):\n",
    "        E_prime = get_edges_subset(karate, tau)\n",
    "\n",
    "        '''Type of perturbation: func = 'betweenness' or func= 'weight'\n",
    "           Object to be pass w.r.t type of perturbation: edge_weights=edge_weights else edge_betweenness = edge_betweenness (just one the other set to None as default)\n",
    "           Type of Normalization: scaling = 'Min-Max' or scaling = 'Sum'\n",
    "           condider only the subset for removal: consider_only_E_prime = True or false\n",
    "        '''\n",
    "        perturbed_adj_matrix = perturb_adj_matrix(\n",
    "            karate, \n",
    "            E_prime, \n",
    "            edge_weights = edge_weights_dict,\n",
    "            edge_betweenness = edge_betweenness_dict, \n",
    "            func = func, \n",
    "            scaling = scaling, \n",
    "            consider_only_E_prime = False\n",
    "        ) \n",
    "        psi_cumm += get_amount_of_change(perturbed_adj_matrix, adj_matrix_karate)\n",
    "        zeta_degree_centrality_cumm += get_deformation_centrality_metric(perturbed_adj_matrix, adj_matrix_karate, degree_centrality) \n",
    "        zeta_katz_centrality_cumm += get_deformation_centrality_metric(perturbed_adj_matrix, adj_matrix_karate, weighted_katz_centrality) \n",
    "        zeta_eigenvector_centrality_cumm += get_deformation_centrality_metric(perturbed_adj_matrix, adj_matrix_karate, weighted_eigenvector_centrality) \n",
    "        \n",
    "    \n",
    "    mean_psi = psi_cumm/n_iter\n",
    "    mean_zeta_degree_centrality = zeta_degree_centrality_cumm/n_iter\n",
    "    mean_zeta_katz_centrality = zeta_katz_centrality_cumm/n_iter\n",
    "    mean_zeta_eigenvector_centrality = zeta_eigenvector_centrality_cumm/n_iter\n",
    "\n",
    "    psi_list_karate.append(mean_psi)\n",
    "    zeta_degree_centrality_list_karate.append(mean_zeta_degree_centrality)\n",
    "    zeta_katz_centrality_list_karate.append(mean_zeta_katz_centrality)\n",
    "    zeta_eigenvector_centrality_list_karate.append(mean_zeta_eigenvector_centrality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ccc58",
   "metadata": {},
   "source": [
    "### Kangaroos graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1379366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = \"betweenness\"\n",
    "scaling = \"Min-Max\"\n",
    "\n",
    "if func == \"betweenness\":\n",
    "    edge_betweenness_dict = edge_betweenness_kangaroos\n",
    "    edge_weights_dict = None\n",
    "elif func == \"weight\":\n",
    "    edge_betweenness_dict = None\n",
    "    edge_weights_dict = edge_weights_kangaroos\n",
    "    \n",
    "\n",
    "psi_list_kangaroos = []\n",
    "zeta_degree_centrality_list_kangaroos = []\n",
    "#zeta_closeness_centrality_list_kangaroos = []\n",
    "zeta_katz_centrality_list_kangaroos = []\n",
    "zeta_eigenvector_centrality_list_kangaroos = []\n",
    "\n",
    "for tau in tau_list:\n",
    "    print(f\"Processing tau: {tau}\")\n",
    "    psi_cumm = 0\n",
    "    zeta_degree_centrality_cumm = 0\n",
    "    zeta_closeness_centrality_cumm = 0\n",
    "    zeta_katz_centrality_cumm = 0\n",
    "    zeta_eigenvector_centrality_cumm = 0\n",
    "    for _ in range(n_iter):\n",
    "        E_prime = get_edges_subset(kangaroos, tau)\n",
    "\n",
    "        '''Type of perturbation: func = 'betweenness' or func= 'weight'\n",
    "           Object to be pass w.r.t type of perturbation: edge_weights=edge_weights else edge_betweenness = edge_betweenness (just one the other set to None as default)\n",
    "           Type of Normalization: scaling = 'Min-Max' or scaling = 'Sum'\n",
    "           condider only the subset for removal: consider_only_E_prime = True or false\n",
    "        '''\n",
    "        perturbed_adj_matrix = perturb_adj_matrix(\n",
    "            kangaroos, \n",
    "            E_prime, \n",
    "            edge_weights = edge_weights_dict,\n",
    "            edge_betweenness = edge_betweenness_dict, \n",
    "            func = func, \n",
    "            scaling = scaling, \n",
    "            consider_only_E_prime = False\n",
    "        ) \n",
    "        psi_cumm += get_amount_of_change(perturbed_adj_matrix, adj_matrix_kangaroos)\n",
    "        zeta_degree_centrality_cumm += get_deformation_centrality_metric(perturbed_adj_matrix, adj_matrix_kangaroos, degree_centrality) \n",
    "        zeta_katz_centrality_cumm += get_deformation_centrality_metric(perturbed_adj_matrix, adj_matrix_kangaroos, weighted_katz_centrality) \n",
    "        zeta_eigenvector_centrality_cumm += get_deformation_centrality_metric(perturbed_adj_matrix, adj_matrix_kangaroos, weighted_eigenvector_centrality) \n",
    "        \n",
    "    \n",
    "    mean_psi = psi_cumm/n_iter\n",
    "    mean_zeta_degree_centrality = zeta_degree_centrality_cumm/n_iter\n",
    "    mean_zeta_katz_centrality = zeta_katz_centrality_cumm/n_iter\n",
    "    mean_zeta_eigenvector_centrality = zeta_eigenvector_centrality_cumm/n_iter\n",
    "\n",
    "    psi_list_kangaroos.append(mean_psi)\n",
    "    zeta_degree_centrality_list_kangaroos.append(mean_zeta_degree_centrality)\n",
    "    zeta_katz_centrality_list_kangaroos.append(mean_zeta_katz_centrality)\n",
    "    zeta_eigenvector_centrality_list_kangaroos.append(mean_zeta_eigenvector_centrality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7395c7-c5a7-40fc-a236-0e1d4133ae2b",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acfaccf7-4af5-4672-88e1-b6cc8b5d2db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"plots/{func.lower()}/{scaling.lower()}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1843d6c0-2dd2-4be2-9909-a1e49f3716f3",
   "metadata": {},
   "source": [
    "## Plot the amount of change (psi) vs. percentage of edges that can fail (tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f26410-0ae8-4974-9b0e-c657568f601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_lists = [psi_list_syn,psi_list_karate,psi_list_kangaroos]\n",
    "plot_results(\n",
    "    x_list = tau_list,\n",
    "    y_lists = psi_lists,\n",
    "    labels = [\"Synthetic\",\"Karate\",\"Kangaroos\"],\n",
    "    x_label = r\"$\\tau$\",\n",
    "    y_label = r\"$\\psi$\",\n",
    "    title = r\"Amount of change vs. percentage of edges that can fail\",\n",
    "    plot_linear_regression = False,\n",
    "    save_path = save_path + f\"tau_vs_psi_niter={n_iter}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0114ca-0700-4c39-90b5-98c8175c2cf1",
   "metadata": {},
   "source": [
    "## Plot the deformation of different centrality metrics (zeta) vs. percentage of edges that can fail (tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70613815-f1af-49c6-a23b-866546159f96",
   "metadata": {},
   "source": [
    "### Degree Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d579fa3f-fc8c-4715-8c41-3bc7ccca169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeta_degree_centrality_lists = [zeta_degree_centrality_list_syn,zeta_degree_centrality_list_karate,zeta_degree_centrality_list_kangaroos]\n",
    "plot_results(\n",
    "    x_list = tau_list,\n",
    "    y_lists = zeta_degree_centrality_lists,\n",
    "    labels = [\"Synthetic\",\"Karate\",\"Kangaroos\"],\n",
    "    x_label = r\"$\\tau$\",\n",
    "    y_label = r\"$\\zeta$\",\n",
    "    title = r\"Deformation of degree centrality vs. percentage of edges that can fail\",\n",
    "    plot_linear_regression = False,\n",
    "    save_path =  save_path + f\"tau_vs_zeta_degree_centrality_niter={n_iter}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb282f0-819c-4a76-a46c-2d81909604d3",
   "metadata": {},
   "source": [
    "### Katz Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0353a70-e012-4b08-a782-effa66011d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeta_katz_centrality_lists = [zeta_katz_centrality_list_syn,zeta_katz_centrality_list_karate,zeta_katz_centrality_list_kangaroos]\n",
    "plot_results(\n",
    "    x_list = tau_list,\n",
    "    y_lists = zeta_katz_centrality_lists,\n",
    "    labels = [\"Synthetic\",\"Karate\",\"Kangaroos\"],\n",
    "    x_label = r\"$\\tau$\",\n",
    "    y_label = r\"$\\zeta$\",\n",
    "    title = r\"Deformation of Katz centrality vs. percentage of edges that can fail\",\n",
    "    plot_linear_regression = False,\n",
    "    save_path =  save_path + f\"tau_vs_zeta_katz_centrality_niter={n_iter}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1886f4-c355-4365-b8ae-821621396748",
   "metadata": {},
   "source": [
    "### Eigenvector Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0859fc72-3244-40af-8abb-73898cdf6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeta_eigenvector_centrality_lists = [zeta_eigenvector_centrality_list_syn,zeta_eigenvector_centrality_list_karate,zeta_eigenvector_centrality_list_kangaroos]\n",
    "plot_results(\n",
    "    x_list = tau_list,\n",
    "    y_lists = zeta_eigenvector_centrality_lists,\n",
    "    labels = [\"Synthetic\",\"Karate\",\"Kangaroos\"],\n",
    "    x_label = r\"$\\tau$\",\n",
    "    y_label = r\"$\\zeta$\",\n",
    "    title = r\"Deformation of Eigenvector centrality vs. percentage of edges that can fail\",\n",
    "    plot_linear_regression = False,\n",
    "    save_path =  save_path + f\"tau_vs_zeta_eigenvector_centrality_niter={n_iter}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988258f-45e0-4d54-a94b-0576a3edbdd6",
   "metadata": {},
   "source": [
    "# Display all plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad78625-ceed-401d-baaa-12cf73471b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_proportional_grid(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
